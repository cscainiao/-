{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结巴分词简单学习使用\n",
    "github地址:https://github.com/fxsjy/jieba\n",
    "\n",
    "https://blog.csdn.net/haishu_zheng/article/details/80430106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装\n",
    "pip install jieba\n",
    "### TF-IDF算法介绍及实现\n",
    "https://blog.csdn.net/asialee_bird/article/details/81486700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要功能\n",
    "### 1、分词\n",
    "结巴中文分词支持的三种分词模式包括： \n",
    "(1) 精确模式：试图将句子最精确地切开，适合文本分析； \n",
    "(2) 全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义问题； \n",
    "(3) 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.798 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[全模式]:  我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "[精确模式]:  我/ 来到/ 北京/ 清华大学\n",
      "[默认模式]:  我/ 来到/ 北京/ 清华大学\n",
      "[搜索引擎模式]:  我/ 来到/ 北京/ 清华/ 华大/ 大学/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 全模式\n",
    "text = \"我来到北京清华大学\"\n",
    "seg_list = jieba.cut(text, cut_all=True)\n",
    "print(u\"[全模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 精确模式\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "print(u\"[精确模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 默认是精确模式\n",
    "seg_list = jieba.cut(text)\n",
    "print(u\"[默认模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(text)\n",
    "print(u\"[搜索引擎模式]: \", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、新词识别\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[新词识别]:  他/ 来到/ 了/ 网易/ 杭研/ 大厦\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "#新词识别  “杭研”并没有在词典中,但是也被Viterbi算法识别出来了\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")\n",
    "print (u\"[新词识别]: \", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、自定义词典\n",
    "先看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[全模式]:  故宫/ 的/ 著名/ 著名景点/ 景点/ 包括/ 乾/ 清宫/ / / 太和/ 太和殿/ 和/ 黄/ 琉璃/ 琉璃瓦/ 等\n",
      "[精确模式]:  故宫/ 的/ 著名景点/ 包括/ 乾/ 清宫/ 、/ 太和殿/ 和/ 黄/ 琉璃瓦/ 等\n",
      "[搜索引擎模式]:  故宫/ 的/ 著名/ 景点/ 著名景点/ 包括/ 乾/ 清宫/ 、/ 太和/ 太和殿/ 和/ 黄/ 琉璃/ 琉璃瓦/ 等\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"故宫的著名景点包括乾清宫、太和殿和黄琉璃瓦等\"\n",
    "\n",
    "# 全模式\n",
    "seg_list = jieba.cut(text, cut_all=True)\n",
    "print(u\"[全模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 精确模式\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "print(u\"[精确模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(text)\n",
    "print(u\"[搜索引擎模式]: \", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，结巴分词工具认出了专有名词”太和殿”，但没有认出”乾清宫”和”黄琉璃瓦”。 \n",
    "也就是说，专有名词”乾清宫”和”黄琉璃瓦”可能因分词而分开，这也是很多分词工具的一个缺陷。 \n",
    "为此，Jieba分词支持开发者使用自定定义的词典，以便包含jieba词库里没有的词语。虽然结巴有新词识别能力，但自行添加新词可以保证更高的正确率，尤其是专有名词。 \n",
    "基本用法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict('dict.txt') #file_name为自定义词典的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[全模式]:  故宫/ 的/ 著名/ 著名景点/ 景点/ 包括/ 乾清宫/ 清宫/ / / 太和/ 太和殿/ 和/ 黄琉璃瓦/ 琉璃/ 琉璃瓦/ 等\n",
      "[精确模式]:  故宫/ 的/ 著名景点/ 包括/ 乾清宫/ 、/ 太和殿/ 和/ 黄琉璃瓦/ 等\n",
      "[搜索引擎模式]:  故宫/ 的/ 著名/ 景点/ 著名景点/ 包括/ 清宫/ 乾清宫/ 、/ 太和/ 太和殿/ 和/ 琉璃/ 琉璃瓦/ 黄琉璃瓦/ 等\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"故宫的著名景点包括乾清宫、太和殿和黄琉璃瓦等\"\n",
    "\n",
    "# 全模式\n",
    "seg_list = jieba.cut(text, cut_all=True)\n",
    "print(u\"[全模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 精确模式\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "print(u\"[精确模式]: \", \"/ \".join(seg_list))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(text)\n",
    "print(u\"[搜索引擎模式]: \", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，新添加的两个专有名词已经被结巴分词工具辨别出来了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、关键词提取\n",
    "在构建VSM向量空间模型过程或者把文本转换成数学形式计算中，你需要运用到关键词提取的技术。\n",
    "\n",
    "基本方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.analyse.extract_tags(sentence, topK) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中sentence为待提取的文本，topK为返回几个TF/IDF权重最大的关键词，默认值为20。\n",
    "\n",
    "程序：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果:\n",
      "故宫/的/著名景点/包括/乾清宫/、/太和殿/和/午门/等/。/其中/乾清宫/非常/精美/，/午门/是/紫禁城/的/正门/，/午门/居中/向阳/。\n",
      "tf-idf算法提取关键词:\n",
      "午门 乾清宫 著名景点 太和殿 向阳\n",
      "TextRank算法提取关键词:\n",
      "乾清宫 午门 著名景点 故宫 包括\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "#导入自定义词典\n",
    "jieba.load_userdict(\"dict.txt\")\n",
    "\n",
    "#精确模式\n",
    "text = \"故宫的著名景点包括乾清宫、太和殿和午门等。其中乾清宫非常精美，午门是紫禁城的正门，午门居中向阳。\"\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "print (u\"分词结果:\")\n",
    "print (\"/\".join(seg_list))\n",
    "\n",
    "#获取关键词\n",
    "tags = jieba.analyse.extract_tags(text, topK=5)      # tf-idf算法提取\n",
    "tags_1 = jieba.analyse.textrank(text,topK=5)       # 基于 TextRank 算法的关键词抽取\n",
    "print (u\"tf-idf算法提取关键词:\")\n",
    "print (\" \".join(tags))\n",
    "\n",
    "print (u\"TextRank算法提取关键词:\")\n",
    "print (\" \".join(tags_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、去除停用词\n",
    "在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，比如“的”、“是”、“而且”、“但是”、”非常“等。这些字或词即被称为Stop Words（停用词）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "故宫著名景点乾清宫、太和殿和午门。其中乾清宫非常精美，午门紫禁城正门。\n",
      "故宫/ 著名景点/ 乾清宫/ 、/ 太和殿/ 和/ 午门/ 。/ 其中/ 乾清宫/ 非常/ 精美/ ，/ 午门/ 紫禁城/ 正门/ 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 去除停用词\n",
    "stopwords = {}.fromkeys(['的', '包括', '等', '是'])\n",
    "text = \"故宫的著名景点包括乾清宫、太和殿和午门等。其中乾清宫非常精美，午门是紫禁城的正门。\"\n",
    "# 精确模式\n",
    "segs = jieba.cut(text, cut_all=False)\n",
    "final = ''\n",
    "for seg in segs:\n",
    "    if seg not in stopwords:\n",
    "            final += seg\n",
    "print (final)\n",
    "\n",
    "seg_list = jieba.cut(final, cut_all=False)\n",
    "print (\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、词性标注\n",
    "jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "\n",
    "标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。\n",
    "\n",
    "用法示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object cut at 0x00000127E6324138>\n",
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "print(words)\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今天就先学习到这，更多用法请查看github示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
